# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

NARA Collection Importer for Arke IPFS - imports National Archives (NARA) collections into an Arke IPFS database using a hierarchical entity structure. The pilot project focuses on the WJC-NSCSW collection (Clinton NSC Speechwriting Office, 2,053 file units, 45,936 digital objects).

## Commands

### Testing and Development

```bash
# Run test import (3 sample records from NARA S3)
cd scripts/nara_import
python test_import_sample.py

# Generate example structure (demonstrates expected output)
python generate_example_structure.py
```

### API Health Check

```bash
# Verify Arke API is running
curl http://localhost:8787
# Expected: {"service":"arke-ipfs-api","version":"0.1.0","status":"ok"}
```

### Working with NARA Data

```bash
# Download sample metadata from S3
aws s3 cp s3://nara-national-archives-catalog/descriptions/collections/coll_WJC-NSCSW/coll_WJC-NSCSW-1.jsonl - --no-sign-request

# List all metadata files (72 JSONL files)
aws s3 ls s3://nara-national-archives-catalog/descriptions/collections/coll_WJC-NSCSW/ --no-sign-request
```

## Architecture

### Data Model - Hierarchical Entity Structure

The importer creates a five-level hierarchy stored in Arke IPFS:

```
Arke Genesis Block (PI: 00000000000000000000000000)
└── Institution (e.g., National Archives)
    └── Collection (e.g., WJC-NSCSW, naId: 7388842)
        └── Series (e.g., Antony Blinken's Files, naId: 7585787)
            └── FileUnit (e.g., "Clinton - Address on Haiti", naId: 23902919)
                └── DigitalObject (e.g., Page 1, objectId: 55251313)
```

Each entity level:
- Has a ULID-based Persistent Identifier (PI) auto-generated by Arke API
- Stores catalog metadata in IPFS as dag-json
- Links to children via `children_pi` arrays in manifest
- Tracks NARA IDs (naId or objectId) for traceability

**Arke Genesis Block**: The root entity (PI: `00000000000000000000000000`) that serves as the origin of the archive tree. All institutions are automatically linked as children of this block when first created. Access via `GET /arke` or `GET /genesis` API endpoints.

### Key Design Decisions

1. **No Image Storage in IPFS**: Digital objects (JPGs/PDFs) are NOT uploaded. Instead, we store S3 URLs, SHA256 hashes, and metadata. This keeps the IPFS database lightweight while preserving content verification.

2. **Streaming Hash Computation**: Files are downloaded in 8KB chunks, hashed on-the-fly, and discarded immediately. This minimizes memory/disk usage when processing thousands of files.

3. **API-Generated PIs**: The Arke API auto-generates ULID PIs. The importer tracks NARA ID → API-generated PI mappings internally.

4. **Catalog Records as Components**: Each entity's metadata is uploaded to IPFS as a JSON catalog record (following schemas in `nara_schema.py`), then the CID is stored in the entity's `components` field.

### Core Modules

**nara_importer.py** (`NARAImporter` class)
- Main orchestration: handles hierarchy creation and parent-child linking
- Tracks NARA ID → PI mappings to avoid duplicate imports
- Methods: `import_institution()`, `import_collection()`, `import_series()`, `import_fileunit()`, `import_digitalobject()`
- Uses CAS (Compare-And-Swap) when appending children to entities
- Automatically links new institutions to Arke genesis block

**arke_api_client.py** (`ArkeClient` class)
- HTTP client for Arke IPFS API (see API_SPEC.md for endpoints)
- Key methods: `create_entity()`, `append_version()`, `upload_json()`, `resolve_pi()`, `get_arke_block()`
- Handles errors: `ArkeAPIError`, `ArkeConflictError`, `ArkeNotFoundError`

**nara_schema.py**
- Catalog record schemas for each entity type (institution, collection, series, fileunit, digitalobject)
- Schema versions: `nara-collection@v1`, `nara-series@v1`, etc.
- Functions return dict structures that get uploaded to IPFS as dag-json

**nara_hash_utils.py**
- `download_and_hash_s3()`: Streaming SHA256 computation for S3 URLs
- `create_content_hash_object()`: Creates standardized hash metadata `{"algorithm": "sha256", "digest_hex": "..."}`
- Raises `HashComputationError` on download/hash failures

**nara_pi.py**
- **NOTE**: Originally designed for semantic PIs like `NARA-WJC-NSCSW-FILE-23902919-{ulid}`, but the API only accepts plain ULIDs
- `generate_pi()`: Returns ULID only (API requirement)
- `generate_semantic_id()`: Creates human-readable IDs for logging (not used as actual PIs)
- `parse_pi()`, `is_valid_pi()`: Utilities for validating semantic PI format

### Data Flow

1. Download NARA metadata from S3 (JSONL format, one record per line)
2. Extract hierarchy: Collection → Series → FileUnit → DigitalObjects
3. For each entity:
   - Create catalog record (dict following schema)
   - Upload catalog to IPFS → get CID
   - Create entity via API (API returns auto-generated PI)
   - Track NARA ID → PI mapping
4. Link parent-child relationships by appending versions with `children_pi_add`
5. For digital objects: download from S3, compute SHA256, store hash + URL (no image upload)

### Relationship Linking Strategy

- Entities are created WITHOUT initial children
- After entity creation, children are linked via `append_version()` with `children_pi_add`
- Uses CAS (expect_tip) to prevent concurrent modification conflicts
- Example: FileUnit created → DigitalObjects created → DigitalObjects added to FileUnit's children

## Configuration

**scripts/nara_import/config/collection_WJC-NSCSW.yaml**
- Collection metadata (naId, date ranges, statistics)
- Series definitions (4 series: Blinken, Boorstin, Orzulak, Rosshirt)
- Import settings (API URL, batch size, retry settings)

## Dependencies

```bash
pip install requests python-ulid
```

Or use `scripts/nara_import/requirements.txt`

## Important Files

- **API_SPEC.md**: Complete Arke IPFS API documentation
- **WJC-NSCSW_Project_Overview.md**: Detailed collection statistics and research context
- **scripts/nara_import/README.md**: Step-by-step usage guide with examples
- **scripts/nara_import/EXAMPLE_STRUCTURE.md**: Shows expected output structure
- **sample_*.json**: Example NARA records for testing

## Common Patterns

### Importing a Record

```python
# 1. Initialize
from arke_api_client import ArkeClient
from nara_importer import NARAImporter

client = ArkeClient("http://localhost:8787")
importer = NARAImporter(client, collection_id="WJC-NSCSW")

# 2. Import hierarchy (importer handles deduplication)
collection_pi = importer.import_collection(
    collection_naid=7388842,
    title="Records of NSC Speechwriting Office",
    date_range={"start": "1993-01-01", "end": "2001-12-31"}
)

series_pi = importer.import_series(
    series_naid=7585787,
    parent_naid=7388842,
    title="Antony Blinken's Files",
    date_range={"start": "1994-01-01", "end": "1998-12-31"}
)

# 3. Import file unit (automatically imports digital objects and links them)
fileunit_pi = importer.import_fileunit(
    fileunit_naid=23902919,
    parent_series_naid=7585787,
    collection_naid=7388842,
    title="Clinton - Address on Haiti 9/15/94",
    record_types=["Textual Records"],
    digital_objects=record["digitalObjects"]  # from NARA metadata
)
```

### Error Handling

```python
try:
    pi = importer.import_fileunit(...)
except ArkeConflictError:
    # Entity already exists, skip or retrieve existing
    pass
except HashComputationError as e:
    # S3 download failed, log and continue
    logger.error(f"Hash failed: {e}")
except ArkeAPIError as e:
    # Generic API error
    logger.error(f"API error: {e}")
```

## API Requirements

The Arke IPFS API must be running at `http://localhost:8787` (configurable in collection YAML). Start the API server before running imports. Verify with: `curl http://localhost:8787`

## Scaling Notes

- Test import processes 3 records (~13-20 digital objects)
- Full WJC-NSCSW collection: 2,053 file units, 45,936 digital objects
- Estimated full import time: 4-8 hours (includes hashing all objects)
- Hash computation is the bottleneck (can parallelize with worker pools)
- API creates entities quickly (metadata-only, no large file uploads)
