# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

NARA Collection Importer for Arke IPFS - imports National Archives (NARA) collections into an Arke IPFS database using a hierarchical entity structure.

## Repository Structure

```
nara_pilot_ingest_v0/
├── API_SPEC.md                      # Complete Arke IPFS API documentation
├── CLAUDE.md                        # This file
├── README.md                        # Main project documentation
└── scripts/
    └── nara_import/                 # NARA import scripts and library
        ├── lib/                     # Core library modules
        │   ├── __init__.py
        │   ├── arke_api_client.py   # Arke IPFS API client
        │   ├── nara_importer.py     # Main importer class
        │   ├── nara_schema.py       # Catalog record schemas
        │   ├── nara_hash_utils.py   # S3 file hashing utilities
        │   └── nara_pi.py           # PI generation utilities
        ├── config/                  # Collection configurations
        │   └── collection_WJC-NSCSW.yaml
        ├── test_import_sample.py    # Test with 3 sample records
        ├── import_limited.py        # Import up to 100 file units
        ├── import_full_collection.py # Full collection import
        ├── requirements.txt         # Python dependencies
        └── README.md                # Usage guide
```

## Quick Start

### Testing and Development

```bash
# Run test import (3 sample records from NARA S3)
cd scripts/nara_import
python test_import_sample.py

# Limited import (100 file units)
python import_limited.py

# Full collection import (2,053 file units)
python import_full_collection.py
```

### API Health Check

```bash
# Verify Arke API is running (production)
curl https://api.arke.institute
# Expected: {"service":"arke-ipfs-api","version":"0.1.0","status":"ok"}

# For local development
curl http://localhost:8787
```

### Working with NARA Data

```bash
# Download sample metadata from S3
aws s3 cp s3://nara-national-archives-catalog/descriptions/collections/coll_WJC-NSCSW/coll_WJC-NSCSW-1.jsonl - --no-sign-request

# List all metadata files (72 JSONL files)
aws s3 ls s3://nara-national-archives-catalog/descriptions/collections/coll_WJC-NSCSW/ --no-sign-request
```

## Architecture

### Data Model - Hierarchical Entity Structure

The importer creates a five-level hierarchy stored in Arke IPFS:

```
Arke Genesis Block (PI: 00000000000000000000000000)
└── Institution (e.g., National Archives)
    └── Collection (e.g., WJC-NSCSW, naId: 7388842)
        └── Series (e.g., Antony Blinken's Files, naId: 7585787)
            └── FileUnit (e.g., "Clinton - Address on Haiti", naId: 23902919)
                └── DigitalObject (e.g., Page 1, objectId: 55251313)
```

Each entity level:
- Has a ULID-based Persistent Identifier (PI) auto-generated by Arke API
- Stores catalog metadata in IPFS as dag-json
- **Bidirectional parent-child links**: Entities link to parents via `parent_pi` field and children via `children_pi` arrays
- Tracks NARA IDs (naId or objectId) for traceability

**Arke Genesis Block**: The root entity (PI: `00000000000000000000000000`) that serves as the origin of the archive tree. All institutions are automatically linked as children of this block when created via the API. Access via `GET /arke` or `GET /genesis` API endpoints.

### Key Design Decisions

1. **No Image Storage in IPFS**: Digital objects (JPGs/PDFs) are NOT uploaded. Instead, we store S3 URLs, SHA256 hashes, and metadata. This keeps the IPFS database lightweight while preserving content verification.

2. **Streaming Hash Computation**: Files are downloaded in 8KB chunks, hashed on-the-fly, and discarded immediately. This minimizes memory/disk usage when processing thousands of files.

3. **API-Generated PIs**: The Arke API auto-generates ULID PIs. The importer tracks NARA ID → API-generated PI mappings internally.

4. **Automatic Bidirectional Relationships**: When creating entities, specify `parent_pi` during creation and the API automatically maintains bidirectional links (sets child's `parent_pi` and updates parent's `children_pi` array).

5. **Catalog Records as Components**: Each entity's metadata is uploaded to IPFS as a JSON catalog record (following schemas in `nara_schema.py`), then the CID is stored in the entity's `components` field.

### Core Library Modules (`scripts/nara_import/lib/`)

**nara_importer.py** (`NARAImporter` class)
- Main orchestration: handles hierarchy creation with automatic parent-child linking
- Tracks NARA ID → API-generated PI mappings to avoid duplicate imports
- Methods: `import_institution()`, `import_collection()`, `import_series()`, `import_fileunit()`, `import_digitalobject()`
- Each import method passes `parent_pi` during entity creation for automatic bidirectional linking
- Institutions are automatically linked to Arke genesis block by the API

**arke_api_client.py** (`ArkeClient` class)
- HTTP client for Arke IPFS API (see API_SPEC.md for endpoints)
- Key methods: `create_entity(parent_pi=...)`, `append_version()`, `upload_json()`, `resolve_pi()`, `get_arke_block()`
- `create_entity()` accepts optional `parent_pi` parameter for automatic bidirectional relationship creation
- Handles errors: `ArkeAPIError`, `ArkeConflictError`, `ArkeNotFoundError`

**nara_schema.py**
- Catalog record schemas for each entity type (institution, collection, series, fileunit, digitalobject)
- Schema versions: `nara-collection@v1`, `nara-series@v1`, etc.
- Functions return dict structures that get uploaded to IPFS as dag-json

**nara_hash_utils.py**
- `download_and_hash_s3()`: Streaming SHA256 computation for S3 URLs
- `create_content_hash_object()`: Creates standardized hash metadata `{"algorithm": "sha256", "digest_hex": "..."}`
- Raises `HashComputationError` on download/hash failures

**nara_pi.py**
- `generate_pi()`: Returns ULID for entity PIs
- `generate_semantic_id()`: Creates human-readable IDs for logging
- `parse_pi()`, `is_valid_pi()`: Utilities for validating PI format

### Data Flow

1. Download NARA metadata from S3 (JSONL format, one record per line)
2. Extract hierarchy: Collection → Series → FileUnit → DigitalObjects
3. For each entity:
   - Create catalog record (dict following schema)
   - Upload catalog to IPFS → get CID
   - Create entity via API with `parent_pi` (API returns auto-generated PI and creates bidirectional link)
   - Track NARA ID → PI mapping
4. For digital objects: download from S3, compute SHA256, store hash + URL (no image upload)

### Relationship Linking Strategy

- **Automatic bidirectional linking**: Entities are created with `parent_pi` specified during creation
- API automatically updates both child (sets `parent_pi`) and parent (appends to `children_pi` array)
- No manual relationship coordination needed - the API handles it
- Example: Create FileUnit with `parent_pi=series_pi` → API links FileUnit to Series in both directions
- Import order: top-down (Institution → Collection → Series → FileUnit → DigitalObject)

## Configuration

**scripts/nara_import/config/collection_WJC-NSCSW.yaml**
- Collection metadata (naId, date ranges, statistics)
- Series definitions (4 series: Blinken, Boorstin, Orzulak, Rosshirt)
- Import settings (API URL, batch size, retry settings)

## Dependencies

```bash
pip install requests python-ulid
```

Or use `scripts/nara_import/requirements.txt`

## Common Patterns

### Importing a Record

```python
# 1. Initialize
from lib import ArkeClient, NARAImporter

# Connect to production API (or set API_BASE_URL environment variable)
client = ArkeClient("https://api.arke.institute")
importer = NARAImporter(client, collection_id="WJC-NSCSW")

# 2. Import institution (links to Arke genesis block automatically)
importer.import_institution(
    name="National Archives",
    description="National Archives and Records Administration",
    url="https://www.archives.gov/"
)

# 3. Import hierarchy (importer handles deduplication)
collection_pi = importer.import_collection(
    collection_naid=7388842,
    title="Records of NSC Speechwriting Office",
    date_range={"start": "1993-01-01", "end": "2001-12-31"}
)

series_pi = importer.import_series(
    series_naid=7585787,
    parent_naid=7388842,
    title="Antony Blinken's Files",
    date_range={"start": "1994-01-01", "end": "1998-12-31"}
)

# 4. Import file unit (automatically imports digital objects and links them)
fileunit_pi = importer.import_fileunit(
    fileunit_naid=23902919,
    parent_series_naid=7585787,
    collection_naid=7388842,
    title="Clinton - Address on Haiti 9/15/94",
    record_types=["Textual Records"],
    digital_objects=record["digitalObjects"]  # from NARA metadata
)
```

### Error Handling

```python
from lib import ArkeConflictError, ArkeAPIError
from lib.nara_hash_utils import HashComputationError

try:
    pi = importer.import_fileunit(...)
except ArkeConflictError:
    # Entity already exists, skip or retrieve existing
    pass
except HashComputationError as e:
    # S3 download failed, log and continue
    logger.error(f"Hash failed: {e}")
except ArkeAPIError as e:
    # Generic API error
    logger.error(f"API error: {e}")
```

## API Requirements

The import scripts connect to the production Arke IPFS API at `https://api.arke.institute` by default. This can be overridden using the `API_BASE_URL` environment variable for local development.

**Production API**: `https://api.arke.institute`
**Local development**: `http://localhost:8787` (set via `export API_BASE_URL=http://localhost:8787`)

Verify API connectivity: `curl https://api.arke.institute`

## Scaling Notes

- Test import processes 3 records (~13-20 digital objects)
- Full WJC-NSCSW collection: 2,053 file units, 45,936 digital objects
- Estimated full import time: 4-8 hours (includes hashing all objects)
- Hash computation is the bottleneck (can parallelize with worker pools)
- API creates entities quickly (metadata-only, no large file uploads)
