# NARA Collection Importer for Arke IPFS

This toolkit imports NARA (National Archives) collections into the Arke IPFS database using a hierarchical entity structure with automatic bidirectional parent-child relationships.

## Overview

The importer creates a five-level hierarchy:

```
Arke Genesis Block (root of all archives)
└── Institution (e.g., National Archives)
    └── Collection (e.g., WJC-NSCSW)
        └── Series (e.g., Antony Blinken's Files)
            └── FileUnit (e.g., "Clinton - Address on Haiti 9/15/94")
                └── DigitalObject (e.g., Page 1, Page 2, ...)
```

Each level is represented as an Arke entity with:
- **Unique PI** (Persistent Identifier) - ULID auto-generated by API
- **Catalog metadata** stored in IPFS as dag-json
- **Bidirectional relationships** - Both `parent_pi` and `children_pi` fields

## Key Features

### 1. **No Image Storage in IPFS**
- Digital objects (JPGs/PDFs) are **NOT** uploaded to IPFS
- Instead, we store:
  - S3 URL (original location)
  - SHA256 hash (content verification)
  - Metadata (filename, size, type, etc.)

**Rationale**: The full collection is ~46K images. Storing S3 URLs + hashes is lightweight and allows future OCR without re-downloading.

### 2. **Automatic Bidirectional Relationships**
- Specify `parent_pi` when creating entities
- API automatically:
  - Sets child's `parent_pi` field
  - Appends child to parent's `children_pi` array
- No manual relationship coordination needed

### 3. **Streaming Hash Computation**
Digital objects are hashed using streaming downloads:
1. Download file in 8KB chunks
2. Compute SHA256 on-the-fly
3. Discard bytes immediately
4. Store only the hash digest

**Benefits**:
- Minimal memory usage (~8KB per file)
- No local disk storage required
- Can process thousands of files without filling disk

## Directory Structure

```
scripts/nara_import/
├── lib/                           # Core library modules
│   ├── __init__.py
│   ├── arke_api_client.py         # Arke IPFS API client
│   ├── nara_importer.py           # Main importer class
│   ├── nara_schema.py             # Catalog record schemas
│   ├── nara_hash_utils.py         # S3 file hashing utilities
│   └── nara_pi.py                 # PI generation utilities
├── config/
│   └── collection_WJC-NSCSW.yaml  # Collection configuration
├── test_import_sample.py          # Test script (3 records)
├── import_limited.py              # Limited import (100 file units)
├── import_full_collection.py      # Full collection import
├── requirements.txt               # Python dependencies
└── README.md                      # This file
```

## Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Or manually:
pip install requests python-ulid
```

## Usage

### 1. Start Arke API

Ensure the Arke IPFS API is running:

```bash
# Verify it's running
curl http://localhost:8787
# Should return: {"service":"arke-ipfs-api","version":"0.1.0","status":"ok"}
```

### 2. Run Test Import (3 Records)

Import 3 sample records to verify everything works:

```bash
python test_import_sample.py
```

**What it does**:
1. Downloads 3 records from NARA S3 bucket
2. Creates Institution entity (National Archives)
3. Creates Collection entity (WJC-NSCSW)
4. Creates Series entities (e.g., Antony Blinken's Files)
5. Creates 3 FileUnit entities
6. Creates ~13 DigitalObject entities
7. Hashes all digital objects (without storing images)
8. Links hierarchy automatically via `parent_pi`

**Expected output**:
```
=== NARA Import Test ===
API: http://localhost:8787
Collection: WJC-NSCSW
Records: 3

=== Import Statistics ===
Institutions created: 1
Collections created:  1
Series created:       2
FileUnits created:    3
DigitalObjects:       13
Bytes hashed:         16,302,937
Errors:               0
========================
```

### 3. Run Limited Import (100 File Units)

Import up to 100 file units for validation:

```bash
python import_limited.py
```

Features:
- Checkpoint/resume capability
- Processes JSONL files sequentially
- Stops after 100 file units
- Logs progress to timestamped file

### 4. Run Full Collection Import

Import the complete WJC-NSCSW collection (2,053 file units, 45,936 digital objects):

```bash
python import_full_collection.py
```

⚠️ **Warning**: This will take 4-8 hours depending on network speed.

### 5. Verify Import

Query the API to verify entities were created:

```bash
# Get Arke genesis block with all institutions
curl http://localhost:8787/arke

# List entities
curl http://localhost:8787/entities?limit=10

# Get specific entity
curl http://localhost:8787/entities/<PI>

# Resolve PI to current tip
curl http://localhost:8787/resolve/<PI>
```

## Catalog Record Schemas

### Institution Catalog Record
```json
{
  "schema": "nara-institution@v1",
  "name": "National Archives",
  "description": "National Archives and Records Administration",
  "url": "https://www.archives.gov/",
  "import_timestamp": "2025-10-12T17:32:46Z"
}
```

### Collection Catalog Record
```json
{
  "schema": "nara-collection@v1",
  "nara_naId": 7388842,
  "collection_identifier": "WJC-NSCSW",
  "title": "Records of NSC Speechwriting Office (Clinton)",
  "date_range": {"start": "1993-01-01", "end": "2001-12-31"},
  "import_timestamp": "2025-10-12T17:32:47Z"
}
```

### Series Catalog Record
```json
{
  "schema": "nara-series@v1",
  "nara_naId": 7585787,
  "parent_naId": 7388842,
  "title": "Antony Blinken's Files",
  "date_range": {"start": "1994-01-01", "end": "1998-12-31"},
  "creators": [...],
  "import_timestamp": "2025-10-12T17:32:48Z"
}
```

### FileUnit Catalog Record
```json
{
  "schema": "nara-fileunit@v1",
  "nara_naId": 23902919,
  "parent_naId": 7585787,
  "collection_naId": 7388842,
  "title": "Clinton - Address on Haiti 9/15/94",
  "level": "fileUnit",
  "record_types": ["Textual Records"],
  "digital_object_count": 11,
  "foia_tracking": "LPWJC 2006-0459-F",
  "access_restriction": {...},
  "import_timestamp": "2025-10-12T17:32:49Z"
}
```

### DigitalObject Catalog Record
```json
{
  "schema": "nara-digitalobject@v1",
  "nara_objectId": "55251313",
  "parent_naId": 23902919,
  "filename": "42_t_7585787_20060459F_002_001_2016_Page_001.JPG",
  "object_type": "Image (JPG)",
  "file_size": 401408,
  "s3_url": "https://s3.amazonaws.com/NARAprodstorage/...",
  "content_hash": {
    "algorithm": "sha256",
    "digest_hex": "a3f58b4c..."
  },
  "page_number": 1,
  "extracted_text": null,
  "import_timestamp": "2025-10-12T17:32:50Z"
}
```

## Python API Usage

```python
from lib import ArkeClient, NARAImporter

# Initialize
client = ArkeClient("http://localhost:8787")
importer = NARAImporter(client, collection_id="WJC-NSCSW")

# Import institution (auto-links to Arke genesis block)
importer.import_institution(
    name="National Archives",
    description="National Archives and Records Administration",
    url="https://www.archives.gov/"
)

# Import collection (auto-links to institution)
collection_pi = importer.import_collection(
    collection_naid=7388842,
    title="Records of NSC Speechwriting Office",
    date_range={"start": "1993-01-01", "end": "2001-12-31"}
)

# Import series (auto-links to collection)
series_pi = importer.import_series(
    series_naid=7585787,
    parent_naid=7388842,
    title="Antony Blinken's Files",
    date_range={"start": "1994-01-01", "end": "1998-12-31"}
)

# Import file unit with digital objects (auto-links to series)
fileunit_pi = importer.import_fileunit(
    fileunit_naid=23902919,
    parent_series_naid=7585787,
    collection_naid=7388842,
    title="Clinton - Address on Haiti 9/15/94",
    record_types=["Textual Records"],
    digital_objects=record["digitalObjects"]
)

# Print statistics
importer.print_stats()
```

## Error Handling

```python
from lib import ArkeConflictError, ArkeAPIError
from lib.nara_hash_utils import HashComputationError

try:
    pi = importer.import_fileunit(...)
except ArkeConflictError:
    # Entity already exists, skip
    pass
except HashComputationError as e:
    # S3 download failed, log and continue
    logger.error(f"Hash failed: {e}")
except ArkeAPIError as e:
    # Generic API error
    logger.error(f"API error: {e}")
```

## Estimated Processing Time

For full WJC-NSCSW collection (45,936 digital objects):

| Component | Time |
|-----------|------|
| Downloading metadata | ~10 minutes |
| Hashing digital objects | ~2-4 hours |
| IPFS uploads (metadata only) | ~1-2 hours |
| Entity creation + linking | ~1-2 hours |
| **Total** | **~4-8 hours** |

## Troubleshooting

### API Connection Errors
```
ArkeAPIError: Request failed: Connection refused
```
**Solution**: Ensure Arke API is running at `http://localhost:8787`

### Hash Computation Errors
```
HashComputationError: Download failed: 404
```
**Solution**: S3 URL may be invalid or file deleted. Script continues with next object.

### Conflict Errors
```
ArkeConflictError: Conflict: PI already exists
```
**Solution**: Entity already imported. Script skips and continues.

## Next Steps

1. ✅ **Run test import** with 3 records
2. ✅ **Verify entities** in Arke API
3. **Scale to limited import** (100 records)
4. **Review results** and validate structure
5. **Full collection import** (2,053 records)
6. **OCR integration** (future phase)

## Questions?

See main project documentation in `/CLAUDE.md` or `/API_SPEC.md`.
